{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/parth-261204/BMI-Calculator/blob/main/MuscleDystrophy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch torchvision"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EQCu4gQxq-Ai",
        "outputId": "c7b058c4-1ff8-4453-df3f-90ea6dffcce1",
        "collapsed": true
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.9.0+cu126)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.24.0+cu126)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.5.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision) (11.3.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nilearn opencv-python scikit-learn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9XW0fYOIrcRC",
        "outputId": "887679e0-38a8-4245-a83e-1d9b7b57ce15",
        "collapsed": true
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nilearn\n",
            "  Downloading nilearn-0.12.1-py3-none-any.whl.metadata (9.9 kB)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.12/dist-packages (4.12.0.88)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from nilearn) (1.5.3)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.12/dist-packages (from nilearn) (6.0.2)\n",
            "Requirement already satisfied: nibabel>=5.2.0 in /usr/local/lib/python3.12/dist-packages (from nilearn) (5.3.3)\n",
            "Requirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.12/dist-packages (from nilearn) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from nilearn) (25.0)\n",
            "Requirement already satisfied: pandas>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from nilearn) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.25.0 in /usr/local/lib/python3.12/dist-packages (from nilearn) (2.32.4)\n",
            "Requirement already satisfied: scipy>=1.8.0 in /usr/local/lib/python3.12/dist-packages (from nilearn) (1.16.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: typing-extensions>=4.6 in /usr/local/lib/python3.12/dist-packages (from nibabel>=5.2.0->nilearn) (4.15.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=2.2.0->nilearn) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=2.2.0->nilearn) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=2.2.0->nilearn) (2025.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.25.0->nilearn) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.25.0->nilearn) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.25.0->nilearn) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.25.0->nilearn) (2025.11.12)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas>=2.2.0->nilearn) (1.17.0)\n",
            "Downloading nilearn-0.12.1-py3-none-any.whl (12.7 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m12.7/12.7 MB\u001b[0m \u001b[31m106.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nilearn\n",
            "Successfully installed nilearn-0.12.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, models\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import cv2\n",
        "from sklearn.model_selection import train_test_split\n",
        "from nilearn.image import resample_to_img\n",
        "from nilearn import datasets, image, masking\n",
        "\n",
        "\n",
        "def apply_clahe(np_img):\n",
        "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n",
        "    lab = cv2.cvtColor(np_img, cv2.COLOR_RGB2LAB)\n",
        "    l, a, b = cv2.split(lab)\n",
        "    cl = clahe.apply(l)\n",
        "    merged = cv2.merge((cl, a, b))\n",
        "    return cv2.cvtColor(merged, cv2.COLOR_LAB2RGB)\n",
        "\n",
        "class MuscularDystrophyDataset(Dataset):\n",
        "    def __init__(self, image_paths, labels, transform=None):\n",
        "        self.image_paths = image_paths\n",
        "        self.labels = labels\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def preprocess_image(self, img_path):\n",
        "        img = Image.open(img_path).convert(\"RGB\")\n",
        "        np_img = np.array(img)\n",
        "\n",
        "        np_img = apply_clahe(np_img)\n",
        "\n",
        "        np_img = np_img.astype(np.float32) / 255.0\n",
        "\n",
        "        img = Image.fromarray((np_img * 255).astype(np.uint8))\n",
        "        return img\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.image_paths[idx]\n",
        "        label = self.labels[idx]\n",
        "        img = self.preprocess_image(img_path)\n",
        "\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "        return img, label"
      ],
      "metadata": {
        "id": "w1UV8clCsJwg"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from collections import defaultdict\n",
        "\n",
        "# Load all image paths & labels\n",
        "def load_image_paths(root_dir):\n",
        "    classes = sorted(os.listdir(os.path.join(root_dir, 'Data')))\n",
        "    image_paths = []\n",
        "    labels = []\n",
        "    for idx, label in enumerate(classes):\n",
        "        folder_path = os.path.join(root_dir, 'Data', label)\n",
        "        for file in os.listdir(folder_path):\n",
        "            if file.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.tiff')):\n",
        "                image_paths.append(os.path.join(folder_path, file))\n",
        "                labels.append(idx)\n",
        "    return image_paths, labels\n",
        "\n",
        "image_paths, labels = load_image_paths(\"./data\")\n",
        "\n",
        "# Stratified split (80-20)\n",
        "train_paths, val_paths, train_labels, val_labels = train_test_split(\n",
        "    image_paths, labels, test_size=0.2, stratify=labels, random_state=42\n",
        ")\n",
        "\n",
        "# Transform (including resizing, flipping, rotation, normalization)\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(10),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.5]*3, [0.5]*3)\n",
        "])\n",
        "\n",
        "train_dataset = MuscularDystrophyDataset(train_paths, train_labels, transform=transform)\n",
        "val_dataset = MuscularDystrophyDataset(val_paths, val_labels, transform=transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32)"
      ],
      "metadata": {
        "id": "4jeYfZYCtMEo",
        "outputId": "6e8569d2-9aab-4dc1-f50d-4fdbd2ffb782",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 326
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: './data/Data'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1050446756.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mimage_paths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mimage_paths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_image_paths\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./data\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m# Stratified split (80-20)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1050446756.py\u001b[0m in \u001b[0;36mload_image_paths\u001b[0;34m(root_dir)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Load all image paths & labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_image_paths\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mclasses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Data'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mimage_paths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './data/Data'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = models.resnet18(pretrained=True)\n",
        "model.fc = nn.Linear(model.fc.in_features, 4)  # 4 classes\n",
        "model = model.to(device)\n",
        "\n",
        "torch.save(model.state_dict(), \"resnet18_dystrophy_model.pth\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cQ90y27wtWVp",
        "outputId": "f8a364bc-3a56-4237-9ae7-5170d08a9028"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 44.7M/44.7M [00:00<00:00, 195MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
        "for epoch in range(10):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    print(f\"\\nüîÅ Starting Epoch {epoch+1}/1\")\n",
        "\n",
        "    for batch_idx, (images, labels) in enumerate(train_loader):\n",
        "        # Move data to the correct device (GPU or CPU)\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        # Forward pass\n",
        "        outputs = model(images)\n",
        "        # Loss computation\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Accumulate loss and accuracy\n",
        "        total_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "        print(f\"  ‚úÖ Batch {batch_idx+1} done | Loss: {loss.item():.4f}\")\n",
        "\n",
        "        # Stop after 3 batches for quick testing (adjust for full training)\n",
        "        if batch_idx == 2:\n",
        "            break\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "    print(f\"‚úÖ Epoch [{epoch+1}/1] Completed | Loss: {total_loss:.4f} | Accuracy: {accuracy:.2f}%\")\n"
      ],
      "metadata": {
        "id": "2uIfK8KTtpb1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install vit-pytorch"
      ],
      "metadata": {
        "id": "f2EwGEFxtvs6",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from vit_pytorch import ViT\n",
        "import torch.nn as nn\n",
        "\n",
        "class ViT_CNN_Hybrid(nn.Module):\n",
        "    def __init__(self, num_classes=4):\n",
        "        super(ViT_CNN_Hybrid, self).__init__()\n",
        "        # CNN backbone (ResNet50)\n",
        "        self.cnn = models.resnet50(pretrained=True)\n",
        "        self.cnn = nn.Sequential(*list(self.cnn.children())[:-2])  # Remove FC and pooling\n",
        "\n",
        "        # Vision Transformer\n",
        "        self.vit = ViT(\n",
        "            image_size=7,       # output from CNN is [B, 2048, 7, 7]\n",
        "            patch_size=1,\n",
        "            num_classes=num_classes,\n",
        "            dim=512,\n",
        "            depth=6,\n",
        "            heads=8,\n",
        "            mlp_dim=1024,\n",
        "            channels=2048,\n",
        "            dropout=0.1,\n",
        "            emb_dropout=0.1\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.cnn(x)  # [B, 2048, 7, 7]\n",
        "        x = self.vit(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "nBHJHhwRKGRV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# ‚úÖ Check for GPU availability and print device info\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    print(\"‚úÖ GPU detected:\", torch.cuda.get_device_name(0))\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print(\"‚ö†Ô∏è Using CPU ‚Äî no GPU detected\")\n",
        "\n",
        "# Initialize the ViT-CNN Hybrid model and move it to the selected device\n",
        "model = ViT_CNN_Hybrid().to(device)\n",
        "\n",
        "# Define loss and optimizer\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
        "\n",
        "# Training loop for 1 epoch\n",
        "for epoch in range(1):  # ‚úÖ Only 1 epoch\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    print(f\"\\nüîÅ Starting Epoch {epoch + 1}/1\")\n",
        "\n",
        "    for batch_idx, (images, labels) in enumerate(train_loader):\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "        # ‚úÖ Minimal batch log every 10 batches\n",
        "        if (batch_idx + 1) % 10 == 0 or (batch_idx + 1) == len(train_loader):\n",
        "            print(f\"  ‚úÖ Batch {batch_idx + 1}/{len(train_loader)} done...\")\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "    print(f\"‚úÖ Epoch [{epoch + 1}/1] Completed | Loss: {total_loss:.4f} | Accuracy: {accuracy:.2f}%\")\n",
        "\n",
        "# ‚úÖ Save the trained model\n",
        "torch.save(model.state_dict(), \"dyst_vit_cnn_hybrid.pth\")\n"
      ],
      "metadata": {
        "id": "xF6Udx8WKKhl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_state_dict(torch.load(\"dyst_vit_cnn_hybrid.pth\"))\n",
        "model.eval()\n",
        "\n",
        "# Evaluation code (e.g., confusion matrix, test set accuracy)"
      ],
      "metadata": {
        "id": "VyiXEQaBQAoK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install timm\n"
      ],
      "metadata": {
        "id": "GkWmodruQE6I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import timm\n",
        "\n",
        "# Load Swin Transformer\n",
        "model = timm.create_model('swin_tiny_patch4_window7_224', pretrained=True, num_classes=4)\n",
        "model = model.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.AdamW(model.parameters(), lr=3e-5, weight_decay=0.01)\n",
        "\n",
        "# Optional: use learning rate scheduler\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10)\n"
      ],
      "metadata": {
        "id": "vMEgRiA8QJBH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(10):  # 10 epochs for quick test\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    print(f\"\\nüåÄ Swin Transformer Epoch {epoch+1}/1\")\n",
        "\n",
        "    for batch_idx, (images, labels) in enumerate(train_loader):\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "        print(f\"  ‚úÖ Batch {batch_idx+1} done | Loss: {loss.item():.4f}\")\n",
        "\n",
        "        if batch_idx == 2:  # Testing mode\n",
        "            break\n",
        "\n",
        "    scheduler.step()\n",
        "    accuracy = 100 * correct / total\n",
        "    print(f\"‚úÖ Epoch [{epoch+1}/10] Completed | Loss: {total_loss:.4f} | Accuracy: {accuracy:.2f}%\")\n"
      ],
      "metadata": {
        "id": "eB_AhsdqQNk3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ‚úÖ Save the trained Swin Transformer model\n",
        "torch.save(model.state_dict(), \"dyst_swin_transformer.pth\")\n",
        "print(\"üíæ Swin Transformer model saved as 'dyst_swin_transformer.pth'\")\n"
      ],
      "metadata": {
        "id": "Q9hvbkHtQZ_L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import timm\n",
        "\n",
        "class MedViTNet(nn.Module):\n",
        "    def __init__(self, num_classes=4):\n",
        "        super(MedViTNet, self).__init__()\n",
        "        # Base Transformer: DeiT\n",
        "        self.transformer = timm.create_model('deit_tiny_patch16_224', pretrained=True, num_classes=0)  # No head\n",
        "\n",
        "        # CNN Branch: basic convolutional features\n",
        "        self.cnn_branch = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.AdaptiveAvgPool2d((7, 7))\n",
        "        )\n",
        "\n",
        "        # Fusion + Classifier\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(self.transformer.num_features + 64 * 7 * 7, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(512, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        cnn_features = self.cnn_branch(x)\n",
        "        cnn_flat = cnn_features.view(x.size(0), -1)\n",
        "\n",
        "        transformer_features = self.transformer(x)\n",
        "\n",
        "        combined = torch.cat([transformer_features, cnn_flat], dim=1)\n",
        "        out = self.fc(combined)\n",
        "        return out\n"
      ],
      "metadata": {
        "id": "52Yx9jlmQhpG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# üì¶ Initialize MedViT-style model\n",
        "model = MedViTNet(num_classes=4).to(device)\n",
        "\n",
        "# ‚öôÔ∏è Loss & Optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
        "\n",
        "# üîÅ Training Loop (1 Epoch)\n",
        "for epoch in range(10):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    print(f\"\\nüîÅ Starting Epoch {epoch+1}/10\")\n",
        "\n",
        "    for batch_idx, (images, labels) in enumerate(train_loader):\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "        print(f\"  ‚úÖ Batch {batch_idx+1} done | Loss: {loss.item():.4f}\")\n",
        "\n",
        "        if batch_idx == 2:\n",
        "            break\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "    print(f\"‚úÖ Epoch [{epoch+1}/1] Completed | Loss: {total_loss:.4f} | Accuracy: {accuracy:.2f}%\")\n"
      ],
      "metadata": {
        "id": "WU05VChLQmRo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), \"dyst_medvit_model.pth\")\n",
        "print(\"üíæ MedViT model saved as 'dyst_medvit_model.pth'\")\n"
      ],
      "metadata": {
        "id": "33cg_tEdQvE-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import timm\n",
        "from torchvision import models\n"
      ],
      "metadata": {
        "id": "j6BKsoSzQy9C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EnsembleModel(nn.Module):\n",
        "    def __init__(self, models, num_classes=4):\n",
        "        super(EnsembleModel, self).__init__()\n",
        "        self.models = models  # A list of models to combine\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Get predictions from each model\n",
        "        outputs = [model(x) for model in self.models]\n",
        "\n",
        "        # Average the outputs (for classification)\n",
        "        avg_output = torch.mean(torch.stack(outputs), dim=0)\n",
        "\n",
        "        return avg_output"
      ],
      "metadata": {
        "id": "5Efn79HsQ2th"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Load ResNet model for classification\n",
        "resnet_model = models.resnet18(pretrained=True)\n",
        "resnet_model.fc = nn.Linear(resnet_model.fc.in_features, 4)  # 4 classes\n",
        "resnet_model.load_state_dict(torch.load(\"resnet18_dystrophy_model.pth\"))\n",
        "resnet_model.eval()\n",
        "\n",
        "# Load ViT model\n",
        "vit_model = ViT_CNN_Hybrid(num_classes=4)\n",
        "vit_model.load_state_dict(torch.load(\"dyst_vit_cnn_hybrid.pth\"))\n",
        "vit_model.eval()\n",
        "\n",
        "# Load Swin Transformer model\n",
        "swin_model = timm.create_model('swin_tiny_patch4_window7_224', pretrained=True, num_classes=4)\n",
        "swin_model.load_state_dict(torch.load(\"dyst_swin_transformer.pth\"))\n",
        "swin_model.eval()\n",
        "\n",
        "# Load MedViT model\n",
        "medvit_model = MedViTNet(num_classes=4)\n",
        "medvit_model.load_state_dict(torch.load(\"dyst_medvit_model.pth\"))\n",
        "medvit_model.eval()\n",
        "\n"
      ],
      "metadata": {
        "id": "mjpkxlExQ5Te"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Combine all models into the ensemble\n",
        "models_list = [resnet_model, vit_model, swin_model, medvit_model]\n",
        "ensemble_model = EnsembleModel(models_list)\n",
        "\n",
        "# Move ensemble model to device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "ensemble_model.to(device)\n",
        "\n",
        "# Define loss and optimizer for ensemble model\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Extract parameters of all individual models and pass them to the optimizer\n",
        "optimizer = optim.Adam(\n",
        "    list(resnet_model.parameters()) +\n",
        "    list(vit_model.parameters()) +\n",
        "    list(swin_model.parameters()) +\n",
        "    list(medvit_model.parameters()), lr=0.0001\n",
        ")"
      ],
      "metadata": {
        "id": "Jq2ux6B6RDuI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and move ResNet model to the device\n",
        "resnet_model = models.resnet18(pretrained=True)\n",
        "resnet_model.fc = nn.Linear(resnet_model.fc.in_features, 4)  # 4 classes\n",
        "resnet_model.load_state_dict(torch.load(\"resnet18_dystrophy_model.pth\", map_location=device))\n",
        "resnet_model.eval()\n",
        "resnet_model.to(device)  # Ensure it's on the same device as the input\n",
        "\n",
        "# Load and move ViT model to the device\n",
        "vit_model = ViT_CNN_Hybrid(num_classes=4)\n",
        "vit_model.load_state_dict(torch.load(\"dyst_vit_cnn_hybrid.pth\", map_location=device))\n",
        "vit_model.eval()\n",
        "vit_model.to(device)  # Ensure it's on the same device as the input\n",
        "\n",
        "# Load and move Swin model to the device\n",
        "swin_model = timm.create_model('swin_tiny_patch4_window7_224', pretrained=True, num_classes=4)\n",
        "swin_model.load_state_dict(torch.load(\"dyst_swin_transformer.pth\", map_location=device))\n",
        "swin_model.eval()\n",
        "swin_model.to(device)  # Ensure it's on the same device as the input\n",
        "\n",
        "# Load and move MedViT model to the device\n",
        "medvit_model = MedViTNet(num_classes=4)\n",
        "medvit_model.load_state_dict(torch.load(\"dyst_medvit_model.pth\", map_location=device))\n",
        "medvit_model.eval()\n",
        "medvit_model.to(device)  # Ensure it's on the same device as the input\n",
        "\n",
        "# Combine all models into the ensemble\n",
        "models_list = [resnet_model, vit_model, swin_model, medvit_model]\n",
        "ensemble_model = EnsembleModel(models_list)\n",
        "\n",
        "# Move ensemble model to device\n",
        "ensemble_model.to(device)\n",
        "\n",
        "# Define loss and optimizer for ensemble model\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Extract parameters of all individual models and pass them to the optimizer\n",
        "optimizer = optim.Adam(\n",
        "    list(resnet_model.parameters()) +\n",
        "    list(vit_model.parameters()) +\n",
        "    list(swin_model.parameters()) +\n",
        "    list(medvit_model.parameters()), lr=0.0001\n",
        ")\n",
        "\n",
        "# Training loop for the ensemble model\n",
        "num_epochs = 1  # Train for only 1 epoch\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    ensemble_model.train()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    print(f\"\\nüîÅ Starting Epoch {epoch+1}/{num_epochs}\")\n",
        "\n",
        "    for batch_idx, (images, labels) in enumerate(train_loader):\n",
        "        # Ensure that both images and labels are moved to the correct device\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        # Forward pass through the ensemble model\n",
        "        outputs = ensemble_model(images)\n",
        "\n",
        "        # Compute the loss\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backpropagation\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()  # Accumulate the total loss\n",
        "        _, predicted = torch.max(outputs.data, 1)  # Get predictions\n",
        "        total += labels.size(0)  # Total number of samples\n",
        "        correct += (predicted == labels).sum().item()  # Count correct predictions\n",
        "\n",
        "        # Print progress every 10 batches or at the last batch\n",
        "        if (batch_idx + 1) % 10 == 0 or (batch_idx + 1) == len(train_loader):\n",
        "            print(f\"  ‚úÖ Batch {batch_idx + 1}/{len(train_loader)} done...\")\n",
        "\n",
        "    # Compute accuracy for the epoch\n",
        "    accuracy = 100 * correct / total\n",
        "    print(f\"‚úÖ Epoch [{epoch+1}/{num_epochs}] Completed | Loss: {total_loss:.4f} | Accuracy: {accuracy:.2f}%\")\n"
      ],
      "metadata": {
        "id": "j6aFEH3ZRH_Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the ensemble model\n",
        "torch.save(ensemble_model.state_dict(), \"dyst_ensemble_model.pth\")\n",
        "print(\"üíæ Ensemble model saved as 'dyst_ensemble_model.pth'\")\n"
      ],
      "metadata": {
        "id": "KBhvIrFAa7Pi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ensemble_model.load_state_dict(torch.load('dyst_ensemble_model.pth', map_location=device))\n"
      ],
      "metadata": {
        "id": "3j2UYHx0bDIv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for model in models_list:\n",
        "    model.to(device)\n",
        "\n",
        "\n",
        "ensemble_model.to(device)\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "predictions = []\n",
        "true_labels = []\n",
        "\n",
        "ensemble_model.eval()  # Set the ensemble model to evaluation mode\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch_idx, (images, labels) in enumerate(val_loader):\n",
        "        # Ensure that both images and labels are moved to the correct device\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        # Get predictions from the ensemble model\n",
        "        outputs = ensemble_model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "        predictions.extend(predicted.cpu().numpy())\n",
        "        true_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "        # Log progress every 10 batches\n",
        "        if (batch_idx + 1) % 10 == 0 or (batch_idx + 1) == len(val_loader):\n",
        "            print(f\"üîÑ Batch {batch_idx + 1}/{len(val_loader)} | Correct: {correct} | Total: {total}\")\n",
        "\n",
        "# Compute final accuracy\n",
        "accuracy = 100 * correct / total\n",
        "print(f\"‚úÖ Validation Accuracy: {accuracy:.2f}%\")\n"
      ],
      "metadata": {
        "id": "U4wdcqI1bFkY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "# Compute Precision, Recall, and F1-Score\n",
        "precision = precision_score(true_labels, predictions, average='weighted')  # Weighted for class imbalance\n",
        "recall = recall_score(true_labels, predictions, average='weighted')\n",
        "f1 = f1_score(true_labels, predictions, average='weighted')\n",
        "\n",
        "print(f\"‚úÖ Precision: {precision:.2f}\")\n",
        "print(f\"‚úÖ Recall: {recall:.2f}\")\n",
        "print(f\"‚úÖ F1-Score: {f1:.2f}\")"
      ],
      "metadata": {
        "id": "UR22UyUrcY1i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Define class labels (based on your dataset's labels)\n",
        "classes = ['Mild Dystrophy', 'Moderate Dystrophy', 'Non Dystrophy', 'Very Mild Dystrophy']\n",
        "\n",
        "# Compute confusion matrix\n",
        "cm = confusion_matrix(true_labels, predictions)\n",
        "\n",
        "# Plot confusion matrix\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=classes, yticklabels=classes)\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('True')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "HQTlSWEtcdcQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_curve, auc\n",
        "from sklearn.preprocessing import label_binarize\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Define class labels\n",
        "classes = ['Mild Dystrophy', 'Moderate Dystrophy', 'Non Dystrophy', 'Very Mild Dystrophy']\n",
        "\n",
        "# Binarize the true labels for multi-class ROC\n",
        "true_labels_bin = label_binarize(true_labels, classes=[0, 1, 2, 3])  # Assuming 4 classes\n",
        "\n",
        "# Calculate ROC curve for each class\n",
        "fpr, tpr, roc_auc = {}, {}, {}\n",
        "for i in range(true_labels_bin.shape[1]):\n",
        "    fpr[i], tpr[i], _ = roc_curve(true_labels_bin[:, i], np.array(predictions) == i)\n",
        "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
        "\n",
        "# Plot ROC curve\n",
        "plt.figure()\n",
        "for i in range(true_labels_bin.shape[1]):\n",
        "    plt.plot(fpr[i], tpr[i], lw=2, label=f'{classes[i]} (area = {roc_auc[i]:.2f})')\n",
        "\n",
        "# Plot diagonal line (chance line)\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "\n",
        "# Customize plot\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Rvtgjz-Qcjpt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "precision = precision_score(true_labels, predictions, average=None)  # Per class\n",
        "recall = recall_score(true_labels, predictions, average=None)  # Per class\n",
        "f1 = f1_score(true_labels, predictions, average=None)  # Per class\n",
        "\n",
        "print(\"Precision per class:\", precision)\n",
        "print(\"Recall per class:\", recall)\n",
        "print(\"F1 Score per class:\", f1)\n",
        "\n",
        "# Macro average (unweighted average of all classes)\n",
        "print(\"Macro Precision:\", precision_score(true_labels, predictions, average='macro'))\n",
        "print(\"Macro Recall:\", recall_score(true_labels, predictions, average='macro'))\n",
        "print(\"Macro F1:\", f1_score(true_labels, predictions, average='macro'))\n",
        "\n",
        "# Weighted average (weighted by the support of each class)\n",
        "print(\"Weighted Precision:\", precision_score(true_labels, predictions, average='weighted'))\n",
        "print(\"Weighted Recall:\", recall_score(true_labels, predictions, average='weighted'))\n",
        "print(\"Weighted F1:\", f1_score(true_labels, predictions, average='weighted'))"
      ],
      "metadata": {
        "id": "FvxO6K8BcuGm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import matthews_corrcoef\n",
        "\n",
        "mcc = matthews_corrcoef(true_labels, predictions)\n",
        "print(f\"Matthews Correlation Coefficient: {mcc}\")"
      ],
      "metadata": {
        "id": "P8k01YeUczg3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "print(classification_report(true_labels, predictions, target_names=classes))"
      ],
      "metadata": {
        "id": "MGYjHQYCcxtQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cumulative_gain(y_true, y_pred):\n",
        "    sorted_idx = np.argsort(y_pred)[::-1]\n",
        "    sorted_true = np.array(y_true)[sorted_idx]\n",
        "    gain = np.cumsum(sorted_true) / np.sum(sorted_true)\n",
        "    return gain\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "gain = cumulative_gain(true_labels, predictions)\n",
        "plt.plot(range(1, len(gain) + 1), gain, label='Cumulative Gain')\n",
        "plt.xlabel('Number of Predictions')\n",
        "plt.ylabel('Cumulative Gain')\n",
        "plt.title('Cumulative Gain Curve')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "BHBjO2Xkc2eG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "precision = precision_score(true_labels, predictions, average=None)\n",
        "recall = recall_score(true_labels, predictions, average=None)\n",
        "f1 = f1_score(true_labels, predictions, average=None)\n",
        "\n",
        "width = 0.2\n",
        "x = np.arange(len(classes))\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.bar(x - width, precision, width, label='Precision')\n",
        "plt.bar(x, recall, width, label='Recall')\n",
        "plt.bar(x + width, f1, width, label='F1 Score')\n",
        "\n",
        "plt.xlabel('Classes')\n",
        "plt.ylabel('Scores')\n",
        "plt.title('Precision, Recall, and F1 Score for Each Class')\n",
        "plt.xticks(x, classes)\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "KYck2Rszc6Ef"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torchvision import models\n",
        "import timm\n",
        "import torch.nn as nn\n",
        "from sklearn.metrics import accuracy_score\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Load the validation dataset\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.5]*3, [0.5]*3)\n",
        "])\n",
        "val_dataset = MuscularDystrophyDataset(val_paths, val_labels, transform=transform)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32)\n",
        "\n",
        "# --- Load Individual Models ---\n",
        "# ResNet18\n",
        "resnet_model = models.resnet18(pretrained=False)\n",
        "resnet_model.fc = nn.Linear(resnet_model.fc.in_features, 4)\n",
        "resnet_model.load_state_dict(torch.load(\"resnet18_dystrophy_model.pth\", map_location=device))\n",
        "resnet_model.eval().to(device)\n",
        "\n",
        "# ViT-CNN Hybrid\n",
        "vit_model = ViT_CNN_Hybrid(num_classes=4)\n",
        "vit_model.load_state_dict(torch.load(\"dyst_vit_cnn_hybrid.pth\", map_location=device))\n",
        "vit_model.eval().to(device)\n",
        "\n",
        "# Swin Transformer\n",
        "swin_model = timm.create_model('swin_tiny_patch4_window7_224', pretrained=False, num_classes=4)\n",
        "swin_model.load_state_dict(torch.load(\"dyst_swin_transformer.pth\", map_location=device))\n",
        "swin_model.eval().to(device)\n",
        "\n",
        "# MedViT\n",
        "medvit_model = MedViTNet(num_classes=4)\n",
        "medvit_model.load_state_dict(torch.load(\"dyst_medvit_model.pth\", map_location=device))\n",
        "medvit_model.eval().to(device)\n",
        "\n",
        "# Ensemble Model\n",
        "ensemble_model = EnsembleModel([resnet_model, vit_model, swin_model, medvit_model])\n",
        "ensemble_model.load_state_dict(torch.load('dyst_ensemble_model.pth', map_location=device))\n",
        "ensemble_model.eval().to(device)\n",
        "\n",
        "models_to_evaluate = {\n",
        "    \"ResNet18\": resnet_model,\n",
        "    \"ViT-CNN Hybrid\": vit_model,\n",
        "    \"Swin Transformer\": swin_model,\n",
        "    \"MedViT\": medvit_model,\n",
        "    \"Ensemble\": ensemble_model\n",
        "}\n",
        "\n",
        "accuracies = {}\n",
        "\n",
        "with torch.no_grad():\n",
        "    for name, model in models_to_evaluate.items():\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        all_predicted = []\n",
        "        all_labels = []\n",
        "        for images, labels in val_loader:\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "            all_predicted.extend(predicted.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "        accuracy = 100 * correct / total\n",
        "        accuracies[name] = accuracy\n",
        "        print(f\"‚úÖ Accuracy of {name}: {accuracy:.2f}%\")\n",
        "\n",
        "# Plotting the accuracies\n",
        "model_names = list(accuracies.keys())\n",
        "accuracy_values = list(accuracies.values())\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.bar(model_names, accuracy_values, color='skyblue')\n",
        "plt.xlabel('Model')\n",
        "plt.ylabel('Accuracy (%)')\n",
        "plt.title('Comparison of Model Accuracies on Validation Set')\n",
        "plt.ylim(0, 100)\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "1qOO47V4gnf-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aXn4z2y5QR7u"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}